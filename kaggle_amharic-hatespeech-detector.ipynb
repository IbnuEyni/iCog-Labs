{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10215941,"sourceType":"datasetVersion","datasetId":6314604}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#installing the datasets package\n!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:28:26.042396Z","iopub.execute_input":"2024-12-17T03:28:26.042823Z","iopub.status.idle":"2024-12-17T03:28:38.211263Z","shell.execute_reply.started":"2024-12-17T03:28:26.042780Z","shell.execute_reply":"2024-12-17T03:28:38.209994Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#installing the transformers package\n!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:29:46.742516Z","iopub.execute_input":"2024-12-17T03:29:46.742939Z","iopub.status.idle":"2024-12-17T03:29:56.791768Z","shell.execute_reply.started":"2024-12-17T03:29:46.742902Z","shell.execute_reply":"2024-12-17T03:29:56.790240Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#installing the evaluate package\n! pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:30:04.694006Z","iopub.execute_input":"2024-12-17T03:30:04.694586Z","iopub.status.idle":"2024-12-17T03:30:15.460583Z","shell.execute_reply.started":"2024-12-17T03:30:04.694529Z","shell.execute_reply":"2024-12-17T03:30:15.459336Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (18.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#importing the datasets package\nfrom datasets import Dataset\nimport datasets\nimport evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:30:19.561842Z","iopub.execute_input":"2024-12-17T03:30:19.562277Z","iopub.status.idle":"2024-12-17T03:30:42.068803Z","shell.execute_reply.started":"2024-12-17T03:30:19.562237Z","shell.execute_reply":"2024-12-17T03:30:42.067405Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\n#import numpy and pandas for mathematical computation and data manipulation respectively\nimport numpy as np\nimport pandas as pd\n#import the pipeline of transformers\nfrom transformers import pipeline\n#import AutoTokenizer for tokenization purposes\nfrom transformers import AutoTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:30:52.521822Z","iopub.execute_input":"2024-12-17T03:30:52.522249Z","iopub.status.idle":"2024-12-17T03:30:52.528453Z","shell.execute_reply.started":"2024-12-17T03:30:52.522210Z","shell.execute_reply":"2024-12-17T03:30:52.527217Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#import torch\nimport torch\n#import Data loader from torch\nfrom torch.utils.data import DataLoader\n#import an optimizer\nfrom torch.optim import AdamW\n#import tqdm for a progress bar\nfrom tqdm.auto import tqdm\n\n\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:30:58.081794Z","iopub.execute_input":"2024-12-17T03:30:58.082215Z","iopub.status.idle":"2024-12-17T03:30:58.087992Z","shell.execute_reply.started":"2024-12-17T03:30:58.082179Z","shell.execute_reply":"2024-12-17T03:30:58.086804Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"\n# Step 3: Import the dataset to be used for Training the model\n\nThe dataset used for this project is an Amharic dataset that was made available by Data Mendeley. It contains Amharic posts and comments retrieved from Facebook. It has 30,000 rows. The dataset can be accessed from [here](https://data.mendeley.com/datasets/ymtmxx385m)\n","metadata":{}},{"cell_type":"code","source":"\n# Paths to the uploaded datasets\ntest_data_path = '/kaggle/input/amhsdatasets/AMHSDataTest.txt'  # Replace with the name of your uploaded test file\ntrain_data_path = '/kaggle/input/amhsdatasets/AMHSDataTrain(1).txt'  # Replace with the name of your uploaded train file\n\n# Load and process the test dataset\nTest = pd.read_csv(test_data_path, header=None, names=['raw'])\nTest['content'] = Test['raw'].apply(lambda x: ','.join(x.split(',')[:-1]))\nTest['label'] = Test['raw'].apply(lambda x: x.split(',')[-1])\n\n# Load and process the train dataset\nTrain = pd.read_csv(train_data_path, header=None, names=['raw'])\nTrain['content'] = Train['raw'].apply(lambda x: ','.join(x.split(',')[:-1]))\nTrain['label'] = Train['raw'].apply(lambda x: x.split(',')[-1])\n\n# Drop the raw column (optional)\nTrain = Train.drop(columns=['raw'])\nTest = Test.drop(columns=['raw'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:05.583547Z","iopub.execute_input":"2024-12-17T03:31:05.583938Z","iopub.status.idle":"2024-12-17T03:31:05.625269Z","shell.execute_reply.started":"2024-12-17T03:31:05.583904Z","shell.execute_reply":"2024-12-17T03:31:05.624231Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Display the processed DataFrame\n(Train.head(-10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:06.502539Z","iopub.execute_input":"2024-12-17T03:31:06.502951Z","iopub.status.idle":"2024-12-17T03:31:06.523783Z","shell.execute_reply.started":"2024-12-17T03:31:06.502916Z","shell.execute_reply":"2024-12-17T03:31:06.522442Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                                content  label\n0                                               Content  Label\n1     ሌቦች የአብይን ፎቶ ከጀርባህ ለጥፈህ ትለፈልፋለህ አብይ ላይ ያለንም እም...    ጥላቻ\n2                     ህዝቤ አንድ ቀን ትግሬን ይበቀላል ጊዜ ዳኛው ቶሎ ና    ጥላቻ\n3                                      ህዝብን የጨርስህ ጅብ ነህ    ጥላቻ\n4                      ህገ መንግስታችን ይከበር ምድረ ጅቦች ሰው በላ ሁላ    ጥላቻ\n...                                                 ...    ...\n1586  አትሌት ሮጦ ቀን ከሌት ሆዳም ዘርፎ ቀን ከሌት ተገናኙ ቦሌ በሕዝብ ፊት ...   መልካም\n1587  አቶ ለማ መገርሳ ጠበል አጥምቆት ቢሄድ እና የአማራውን ጥቅም እንዴት ማስ...   መልካም\n1588  አንተ ለህሊናህ ሞተህ ለሆድክ የተገዛ አሸናፊው ህዝብ እንጅ ጠመንጃ አይደ...   መልካም\n1589  አንተ ሰው ታረም ሰካራም ነህ እያበድክ ነውና ታረም ሲጋራ አጫሽ ነህ እየ...   መልካም\n1590  አንድ ድፎ ለብቻው የሚበላ ሆዳም ነህ ለማለት ነው በጋበዝኩ እጄን ተነከስ...   መልካም\n\n[1591 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Content</td>\n      <td>Label</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ሌቦች የአብይን ፎቶ ከጀርባህ ለጥፈህ ትለፈልፋለህ አብይ ላይ ያለንም እም...</td>\n      <td>ጥላቻ</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ህዝቤ አንድ ቀን ትግሬን ይበቀላል ጊዜ ዳኛው ቶሎ ና</td>\n      <td>ጥላቻ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ህዝብን የጨርስህ ጅብ ነህ</td>\n      <td>ጥላቻ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ህገ መንግስታችን ይከበር ምድረ ጅቦች ሰው በላ ሁላ</td>\n      <td>ጥላቻ</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1586</th>\n      <td>አትሌት ሮጦ ቀን ከሌት ሆዳም ዘርፎ ቀን ከሌት ተገናኙ ቦሌ በሕዝብ ፊት ...</td>\n      <td>መልካም</td>\n    </tr>\n    <tr>\n      <th>1587</th>\n      <td>አቶ ለማ መገርሳ ጠበል አጥምቆት ቢሄድ እና የአማራውን ጥቅም እንዴት ማስ...</td>\n      <td>መልካም</td>\n    </tr>\n    <tr>\n      <th>1588</th>\n      <td>አንተ ለህሊናህ ሞተህ ለሆድክ የተገዛ አሸናፊው ህዝብ እንጅ ጠመንጃ አይደ...</td>\n      <td>መልካም</td>\n    </tr>\n    <tr>\n      <th>1589</th>\n      <td>አንተ ሰው ታረም ሰካራም ነህ እያበድክ ነውና ታረም ሲጋራ አጫሽ ነህ እየ...</td>\n      <td>መልካም</td>\n    </tr>\n    <tr>\n      <th>1590</th>\n      <td>አንድ ድፎ ለብቻው የሚበላ ሆዳም ነህ ለማለት ነው በጋበዝኩ እጄን ተነከስ...</td>\n      <td>መልካም</td>\n    </tr>\n  </tbody>\n</table>\n<p>1591 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"Test.head(-10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:08.163051Z","iopub.execute_input":"2024-12-17T03:31:08.163454Z","iopub.status.idle":"2024-12-17T03:31:08.175474Z","shell.execute_reply.started":"2024-12-17T03:31:08.163417Z","shell.execute_reply":"2024-12-17T03:31:08.174390Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                               content  \\\n0                                              Content   \n1    ዘረኛ ሰው ነው ስለሌላ ዘር ቀን ከለሊት ሲለፈልፍ የሚውለው  ዘረኛ መሆን...   \n2    ዘፈኑ ጥሩ ነው አንድ ሆዳም አማራ ግን የትግራይን የሽንት ጨርቅ በረጅም ...   \n3    ዛሬ ቅማላም ትግሬዎች በህዝባችን የተተፉና በገጀራ ዋጋቸውን የሚያገኙበት ...   \n4    ዝባዝንኬ አንዳንድ ኤፍ ኤም ሬድዮ ጣቢያዎች ላይ የሚተላለፉ ሸርሙጣ ሸርሙ...   \n..                                                 ...   \n386  ሃሌ ሉያ  ቸር ነውና  ምህረቱም ለዘላለም ነውና እግዚአብሔርን አመስግኑ ...   \n387                                                      \n388  የዳዊት የምስጋና መዝሙር   ልቤ ጽኑ ነው  አቤቱ  ልቤ ጽኑ ነው  እቀኛ...   \n389  ለመዘምራን አለቃ  የዳዊት መዝሙር   አምላክ ሆይ  ምሥጋናዬን ዝም አትበ...   \n390  የዳዊት መዝሙር እግዚአብሔር ጌታዬን  ጠላቶችህን ለእግርህ መቀመጫ እስካደ...   \n\n                                                 label  \n0                                                Label  \n1                                                  ጥላቻ  \n2                                                 ጥላቻ   \n3                                                  ጥላቻ  \n4                                                  ጥላቻ  \n..                                                 ...  \n386                                              መልካም   \n387  ሃሌ ሉያ ቸር ነውና ምሕረቱ ለዘላለም ነውና እግዚአብሔርን አመስግኑ እግዚ...  \n388                                               መልካም  \n389                                              መልካም   \n390                                               መልካም  \n\n[391 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Content</td>\n      <td>Label</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ዘረኛ ሰው ነው ስለሌላ ዘር ቀን ከለሊት ሲለፈልፍ የሚውለው  ዘረኛ መሆን...</td>\n      <td>ጥላቻ</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ዘፈኑ ጥሩ ነው አንድ ሆዳም አማራ ግን የትግራይን የሽንት ጨርቅ በረጅም ...</td>\n      <td>ጥላቻ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ዛሬ ቅማላም ትግሬዎች በህዝባችን የተተፉና በገጀራ ዋጋቸውን የሚያገኙበት ...</td>\n      <td>ጥላቻ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ዝባዝንኬ አንዳንድ ኤፍ ኤም ሬድዮ ጣቢያዎች ላይ የሚተላለፉ ሸርሙጣ ሸርሙ...</td>\n      <td>ጥላቻ</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>386</th>\n      <td>ሃሌ ሉያ  ቸር ነውና  ምህረቱም ለዘላለም ነውና እግዚአብሔርን አመስግኑ ...</td>\n      <td>መልካም</td>\n    </tr>\n    <tr>\n      <th>387</th>\n      <td></td>\n      <td>ሃሌ ሉያ ቸር ነውና ምሕረቱ ለዘላለም ነውና እግዚአብሔርን አመስግኑ እግዚ...</td>\n    </tr>\n    <tr>\n      <th>388</th>\n      <td>የዳዊት የምስጋና መዝሙር   ልቤ ጽኑ ነው  አቤቱ  ልቤ ጽኑ ነው  እቀኛ...</td>\n      <td>መልካም</td>\n    </tr>\n    <tr>\n      <th>389</th>\n      <td>ለመዘምራን አለቃ  የዳዊት መዝሙር   አምላክ ሆይ  ምሥጋናዬን ዝም አትበ...</td>\n      <td>መልካም</td>\n    </tr>\n    <tr>\n      <th>390</th>\n      <td>የዳዊት መዝሙር እግዚአብሔር ጌታዬን  ጠላቶችህን ለእግርህ መቀመጫ እስካደ...</td>\n      <td>መልካም</td>\n    </tr>\n  </tbody>\n</table>\n<p>391 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"Train['label'] = Train['label'].replace(['መልካም', 'ጥላቻ ','ጥላቻ'],[0,1,1])\n(Train.head(-10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:09.702814Z","iopub.execute_input":"2024-12-17T03:31:09.703192Z","iopub.status.idle":"2024-12-17T03:31:09.718085Z","shell.execute_reply.started":"2024-12-17T03:31:09.703159Z","shell.execute_reply":"2024-12-17T03:31:09.716821Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                                content  label\n0                                               Content  Label\n1     ሌቦች የአብይን ፎቶ ከጀርባህ ለጥፈህ ትለፈልፋለህ አብይ ላይ ያለንም እም...      1\n2                     ህዝቤ አንድ ቀን ትግሬን ይበቀላል ጊዜ ዳኛው ቶሎ ና      1\n3                                      ህዝብን የጨርስህ ጅብ ነህ      1\n4                      ህገ መንግስታችን ይከበር ምድረ ጅቦች ሰው በላ ሁላ      1\n...                                                 ...    ...\n1586  አትሌት ሮጦ ቀን ከሌት ሆዳም ዘርፎ ቀን ከሌት ተገናኙ ቦሌ በሕዝብ ፊት ...      0\n1587  አቶ ለማ መገርሳ ጠበል አጥምቆት ቢሄድ እና የአማራውን ጥቅም እንዴት ማስ...      0\n1588  አንተ ለህሊናህ ሞተህ ለሆድክ የተገዛ አሸናፊው ህዝብ እንጅ ጠመንጃ አይደ...      0\n1589  አንተ ሰው ታረም ሰካራም ነህ እያበድክ ነውና ታረም ሲጋራ አጫሽ ነህ እየ...      0\n1590  አንድ ድፎ ለብቻው የሚበላ ሆዳም ነህ ለማለት ነው በጋበዝኩ እጄን ተነከስ...      0\n\n[1591 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Content</td>\n      <td>Label</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ሌቦች የአብይን ፎቶ ከጀርባህ ለጥፈህ ትለፈልፋለህ አብይ ላይ ያለንም እም...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ህዝቤ አንድ ቀን ትግሬን ይበቀላል ጊዜ ዳኛው ቶሎ ና</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ህዝብን የጨርስህ ጅብ ነህ</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ህገ መንግስታችን ይከበር ምድረ ጅቦች ሰው በላ ሁላ</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1586</th>\n      <td>አትሌት ሮጦ ቀን ከሌት ሆዳም ዘርፎ ቀን ከሌት ተገናኙ ቦሌ በሕዝብ ፊት ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1587</th>\n      <td>አቶ ለማ መገርሳ ጠበል አጥምቆት ቢሄድ እና የአማራውን ጥቅም እንዴት ማስ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1588</th>\n      <td>አንተ ለህሊናህ ሞተህ ለሆድክ የተገዛ አሸናፊው ህዝብ እንጅ ጠመንጃ አይደ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1589</th>\n      <td>አንተ ሰው ታረም ሰካራም ነህ እያበድክ ነውና ታረም ሲጋራ አጫሽ ነህ እየ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1590</th>\n      <td>አንድ ድፎ ለብቻው የሚበላ ሆዳም ነህ ለማለት ነው በጋበዝኩ እጄን ተነከስ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1591 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"Test['label'] = Test['label'].replace(['መልካም', 'መልካም', 'መልካም', 'ጥላቻ ','ጥላቻ'], [0, 0, 0, 1,  1])\nTest.head(-10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:11.876478Z","iopub.execute_input":"2024-12-17T03:31:11.876891Z","iopub.status.idle":"2024-12-17T03:31:11.891260Z","shell.execute_reply.started":"2024-12-17T03:31:11.876852Z","shell.execute_reply":"2024-12-17T03:31:11.889979Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                               content  \\\n0                                              Content   \n1    ዘረኛ ሰው ነው ስለሌላ ዘር ቀን ከለሊት ሲለፈልፍ የሚውለው  ዘረኛ መሆን...   \n2    ዘፈኑ ጥሩ ነው አንድ ሆዳም አማራ ግን የትግራይን የሽንት ጨርቅ በረጅም ...   \n3    ዛሬ ቅማላም ትግሬዎች በህዝባችን የተተፉና በገጀራ ዋጋቸውን የሚያገኙበት ...   \n4    ዝባዝንኬ አንዳንድ ኤፍ ኤም ሬድዮ ጣቢያዎች ላይ የሚተላለፉ ሸርሙጣ ሸርሙ...   \n..                                                 ...   \n386  ሃሌ ሉያ  ቸር ነውና  ምህረቱም ለዘላለም ነውና እግዚአብሔርን አመስግኑ ...   \n387                                                      \n388  የዳዊት የምስጋና መዝሙር   ልቤ ጽኑ ነው  አቤቱ  ልቤ ጽኑ ነው  እቀኛ...   \n389  ለመዘምራን አለቃ  የዳዊት መዝሙር   አምላክ ሆይ  ምሥጋናዬን ዝም አትበ...   \n390  የዳዊት መዝሙር እግዚአብሔር ጌታዬን  ጠላቶችህን ለእግርህ መቀመጫ እስካደ...   \n\n                                                 label  \n0                                                Label  \n1                                                    1  \n2                                                    1  \n3                                                    1  \n4                                                    1  \n..                                                 ...  \n386                                              መልካም   \n387  ሃሌ ሉያ ቸር ነውና ምሕረቱ ለዘላለም ነውና እግዚአብሔርን አመስግኑ እግዚ...  \n388                                                  0  \n389                                              መልካም   \n390                                                  0  \n\n[391 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Content</td>\n      <td>Label</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ዘረኛ ሰው ነው ስለሌላ ዘር ቀን ከለሊት ሲለፈልፍ የሚውለው  ዘረኛ መሆን...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ዘፈኑ ጥሩ ነው አንድ ሆዳም አማራ ግን የትግራይን የሽንት ጨርቅ በረጅም ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ዛሬ ቅማላም ትግሬዎች በህዝባችን የተተፉና በገጀራ ዋጋቸውን የሚያገኙበት ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ዝባዝንኬ አንዳንድ ኤፍ ኤም ሬድዮ ጣቢያዎች ላይ የሚተላለፉ ሸርሙጣ ሸርሙ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>386</th>\n      <td>ሃሌ ሉያ  ቸር ነውና  ምህረቱም ለዘላለም ነውና እግዚአብሔርን አመስግኑ ...</td>\n      <td>መልካም</td>\n    </tr>\n    <tr>\n      <th>387</th>\n      <td></td>\n      <td>ሃሌ ሉያ ቸር ነውና ምሕረቱ ለዘላለም ነውና እግዚአብሔርን አመስግኑ እግዚ...</td>\n    </tr>\n    <tr>\n      <th>388</th>\n      <td>የዳዊት የምስጋና መዝሙር   ልቤ ጽኑ ነው  አቤቱ  ልቤ ጽኑ ነው  እቀኛ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>389</th>\n      <td>ለመዘምራን አለቃ  የዳዊት መዝሙር   አምላክ ሆይ  ምሥጋናዬን ዝም አትበ...</td>\n      <td>መልካም</td>\n    </tr>\n    <tr>\n      <th>390</th>\n      <td>የዳዊት መዝሙር እግዚአብሔር ጌታዬን  ጠላቶችህን ለእግርህ መቀመጫ እስካደ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>391 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"\n# Step 4: Preprocess the Dataset\n\nWhen the dataset was retrived, the labels and the post were in different files.\n\n  - Hence, the first step in this phase is merging the files into one panda's dataframe.\n  - Second step is Label encoding. Lable encoding is the process of converting the labels(classes) into numeric format to make it easier for the machine to understand it\n  - Third step is dividing the dataset into training, validation and testing categories. The division ratio is 7:1:2 respectively.\n  - Last step is to remove an unncessary columns from the main dataset and merging the all the categories into one main dataset\n\n","metadata":{}},{"cell_type":"code","source":"Test['label'] = Test['label'].replace({'መልካም': 0})\nTest['label'] = Test['label'].replace({'ጥላቻ': 1})\nTest = Test[Test['label'] != 'Label']\nTest['label'] = pd.to_numeric(Test['label'], errors='coerce')  # Invalid entries become NaN\nTest = Test.dropna(subset=['label'])\nTest['label'] = Test['label'].astype(int)\nTest.head(-10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:13.342481Z","iopub.execute_input":"2024-12-17T03:31:13.342991Z","iopub.status.idle":"2024-12-17T03:31:13.365204Z","shell.execute_reply.started":"2024-12-17T03:31:13.342953Z","shell.execute_reply":"2024-12-17T03:31:13.363735Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                               content  label\n1    ዘረኛ ሰው ነው ስለሌላ ዘር ቀን ከለሊት ሲለፈልፍ የሚውለው  ዘረኛ መሆን...      1\n2    ዘፈኑ ጥሩ ነው አንድ ሆዳም አማራ ግን የትግራይን የሽንት ጨርቅ በረጅም ...      1\n3    ዛሬ ቅማላም ትግሬዎች በህዝባችን የተተፉና በገጀራ ዋጋቸውን የሚያገኙበት ...      1\n4    ዝባዝንኬ አንዳንድ ኤፍ ኤም ሬድዮ ጣቢያዎች ላይ የሚተላለፉ ሸርሙጣ ሸርሙ...      1\n5    የሀበሻ ምግብ ሂጄ ቅማላም ትግሬዎች ሲንጫጩ አበሸቁኝ እኛ በስንት ቀን አ...      1\n..                                                 ...    ...\n380  ስለ አማራው ግፍ እና ጭቆና የማያወራ ስለመፍትሄውም የማያስብ ሰው እርሱ ...      0\n381  የዳዊት መዝሙር   አቤቱ  ምሕረትንና ፍርድን እቀኝልሃለሁ እዘምራለሁ  ን...      0\n382  ባዘነና ልመናውን በእግዚአብሔር ፊት ባፈሰሰ ጊዜ የችግረኛ ጸሎት   አቤቱ...      0\n383  የዳዊት መዝሙር   ነፍሴ ሆይ  እግዚአብሔርን ባርኪ  አጥንቶቼም ሁሉ  የ...      0\n384  የዳዊት መዝሙር   ነፍሴ ሆይ  እግዚአብሔርን ባርኪ  አቤቱ አምላኬ ሆይ ...      0\n\n[371 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>ዘረኛ ሰው ነው ስለሌላ ዘር ቀን ከለሊት ሲለፈልፍ የሚውለው  ዘረኛ መሆን...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ዘፈኑ ጥሩ ነው አንድ ሆዳም አማራ ግን የትግራይን የሽንት ጨርቅ በረጅም ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ዛሬ ቅማላም ትግሬዎች በህዝባችን የተተፉና በገጀራ ዋጋቸውን የሚያገኙበት ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ዝባዝንኬ አንዳንድ ኤፍ ኤም ሬድዮ ጣቢያዎች ላይ የሚተላለፉ ሸርሙጣ ሸርሙ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>የሀበሻ ምግብ ሂጄ ቅማላም ትግሬዎች ሲንጫጩ አበሸቁኝ እኛ በስንት ቀን አ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>380</th>\n      <td>ስለ አማራው ግፍ እና ጭቆና የማያወራ ስለመፍትሄውም የማያስብ ሰው እርሱ ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>381</th>\n      <td>የዳዊት መዝሙር   አቤቱ  ምሕረትንና ፍርድን እቀኝልሃለሁ እዘምራለሁ  ን...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>ባዘነና ልመናውን በእግዚአብሔር ፊት ባፈሰሰ ጊዜ የችግረኛ ጸሎት   አቤቱ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>383</th>\n      <td>የዳዊት መዝሙር   ነፍሴ ሆይ  እግዚአብሔርን ባርኪ  አጥንቶቼም ሁሉ  የ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>384</th>\n      <td>የዳዊት መዝሙር   ነፍሴ ሆይ  እግዚአብሔርን ባርኪ  አቤቱ አምላኬ ሆይ ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>371 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# trainn_dataset, evaluat_dataset = train_test_split(Train, test_size=0.75, random_state=42)\ntrain_dataset, evaluation_dataset = train_test_split(Train, test_size=0.20, random_state=42)\ntest_dataset, eval_test_dataset = train_test_split(Test, test_size=0.001, random_state=42)\nprint('Training dataset shape: ', train_dataset.shape)\nprint('Validation dataset shape: ', evaluation_dataset.shape)\nprint('Testing dataset shape: ', test_dataset.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:14.794385Z","iopub.execute_input":"2024-12-17T03:31:14.794794Z","iopub.status.idle":"2024-12-17T03:31:14.806272Z","shell.execute_reply.started":"2024-12-17T03:31:14.794758Z","shell.execute_reply":"2024-12-17T03:31:14.805165Z"}},"outputs":[{"name":"stdout","text":"Training dataset shape:  (1280, 2)\nValidation dataset shape:  (321, 2)\nTesting dataset shape:  (380, 2)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"#convert format of the dataset to HuggingFace Dataset from Pandas DataFrame\ntrain_dataset['label'] = train_dataset['label'].replace({'መልካም': 0})\ntrain_dataset['label'] = train_dataset['label'].replace({'ጥላቻ': 1})\ntrain_dataset = train_dataset[train_dataset['label'] != 'Label']\ntrain_dataset['label'] = pd.to_numeric(train_dataset['label'], errors='coerce')  # Invalid entries become NaN\ntrain_dataset = train_dataset.dropna(subset=['label'])\ntrain_dataset['label'] = train_dataset['label'].astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:15.062568Z","iopub.execute_input":"2024-12-17T03:31:15.063023Z","iopub.status.idle":"2024-12-17T03:31:15.077001Z","shell.execute_reply.started":"2024-12-17T03:31:15.062986Z","shell.execute_reply":"2024-12-17T03:31:15.075792Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"#convert format of the dataset to HuggingFace Dataset from Pandas DataFrame\nevaluation_dataset['label'] = evaluation_dataset['label'].replace({'መልካም': 0})\nevaluation_dataset['label'] = evaluation_dataset['label'].replace({'ጥላቻ': 1})\nevaluation_dataset = evaluation_dataset[evaluation_dataset['label'] != 'Label']\nevaluation_dataset['label'] = pd.to_numeric(evaluation_dataset['label'], errors='coerce')  # Invalid entries become NaN\nevaluation_dataset = evaluation_dataset.dropna(subset=['label'])\nevaluation_dataset['label'] = evaluation_dataset['label'].astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:15.622580Z","iopub.execute_input":"2024-12-17T03:31:15.623016Z","iopub.status.idle":"2024-12-17T03:31:15.635606Z","shell.execute_reply.started":"2024-12-17T03:31:15.622977Z","shell.execute_reply":"2024-12-17T03:31:15.634656Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"#convert format of the dataset to HuggingFace Dataset from Pandas DataFrame\ntest_dataset=Dataset.from_pandas(test_dataset)\n\n#convert the format of the dataset to HuggingFace Dataset from Pandas DataFrame\ntrain_dataset=Dataset.from_pandas(train_dataset)\n\n#convert the format of the dataset to HuggingFace Dataset from Pandas DataFrame\nevaluation_dataset=Dataset.from_pandas(evaluation_dataset)\n\n#preview of the dataset after conversion\n(test_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:15.902495Z","iopub.execute_input":"2024-12-17T03:31:15.903486Z","iopub.status.idle":"2024-12-17T03:31:15.958455Z","shell.execute_reply.started":"2024-12-17T03:31:15.903447Z","shell.execute_reply":"2024-12-17T03:31:15.957325Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['content', 'label', '__index_level_0__'],\n    num_rows: 380\n})"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"#combine the train and test dataset into one datset\nmain_dataset= datasets.DatasetDict({\n    'train': train_dataset,\n    'test': test_dataset,\n    'evaluate': evaluation_dataset\n})\n\n#preview of the dataset after merging\nmain_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:17.463601Z","iopub.execute_input":"2024-12-17T03:31:17.464442Z","iopub.status.idle":"2024-12-17T03:31:17.471847Z","shell.execute_reply.started":"2024-12-17T03:31:17.464390Z","shell.execute_reply":"2024-12-17T03:31:17.470682Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['content', 'label', '__index_level_0__'],\n        num_rows: 1214\n    })\n    test: Dataset({\n        features: ['content', 'label', '__index_level_0__'],\n        num_rows: 380\n    })\n    evaluate: Dataset({\n        features: ['content', 'label', '__index_level_0__'],\n        num_rows: 301\n    })\n})"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# training and testing data size\ntraining_data_size = main_dataset['train'].num_rows\ntesting_data_size = main_dataset['test'].num_rows\nevaluation_data_size = main_dataset['evaluate'].num_rows","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:17.726784Z","iopub.execute_input":"2024-12-17T03:31:17.727268Z","iopub.status.idle":"2024-12-17T03:31:17.732822Z","shell.execute_reply.started":"2024-12-17T03:31:17.727228Z","shell.execute_reply":"2024-12-17T03:31:17.731683Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# Step 5: Tokenizing Dataset\n\n\nA Tokenizer is used to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data.\n\nIn this case, the tokenizer used is an AutoTokenizer from the fine-tuned mBERT model made available by Hugging face here\n\nIn this phase, we have the following tasks:\n\n- Load the tokenizer\n- Create a tokenizer function that takes the dataset in batches and tokenize them using the tokenizer loaded from the model\n- Call the tokenizer function on the whole dataset\n\n","metadata":{}},{"cell_type":"code","source":"# Specify a custom cache directory\ntokenizer = AutoTokenizer.from_pretrained(\n    \"Davlan/bert-base-multilingual-cased-finetuned-amharic\",\n    cache_dir=\"/kaggle/working/\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:21.762953Z","iopub.execute_input":"2024-12-17T03:31:21.763390Z","iopub.status.idle":"2024-12-17T03:31:22.905140Z","shell.execute_reply.started":"2024-12-17T03:31:21.763337Z","shell.execute_reply":"2024-12-17T03:31:22.903926Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/335 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"128c312d9db946c4a458f6c35b4776da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/798 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b72d4da12a64ccaa399298e9dc5872b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.55M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3628f4535c24561bd49b3f0c01f211e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20d3ba5b38a84964885b166fcfa3aa93"}},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"#loading a tokenizer from the pretrained model\ntokenizer = AutoTokenizer.from_pretrained(\"Davlan/bert-base-multilingual-cased-finetuned-amharic\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:24.402605Z","iopub.execute_input":"2024-12-17T03:31:24.403014Z","iopub.status.idle":"2024-12-17T03:31:25.378101Z","shell.execute_reply.started":"2024-12-17T03:31:24.402978Z","shell.execute_reply":"2024-12-17T03:31:25.376987Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/335 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6655122a66474fd4adc4c8823847e04f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/798 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"264808e16abf4130a15350508cc8cbc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.55M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"824043a542d741df910e12d5bf0cab0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efb879ad7bda4e3593fd60854ae5481e"}},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"#Have a tokenizer function that uses the tokenizer\ndef tokenize_function(data):\n    return tokenizer(data[\"content\"], padding=\"max_length\", truncation=True)\n\n\n#Tokenize all the data using the mapping functionality\ntokenized_datasets = main_dataset.map(tokenize_function)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:26.682450Z","iopub.execute_input":"2024-12-17T03:31:26.682905Z","iopub.status.idle":"2024-12-17T03:31:28.456066Z","shell.execute_reply.started":"2024-12-17T03:31:26.682866Z","shell.execute_reply":"2024-12-17T03:31:28.454931Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1214 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dd5706fc3124bf4b72cf46d7573a13e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/380 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6f9e621ee474c5ebf62da2ab670aa27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/301 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"492ae9bb5ae442dfa519690bba17b64f"}},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"#empty cache\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:34.303551Z","iopub.execute_input":"2024-12-17T03:31:34.304287Z","iopub.status.idle":"2024-12-17T03:31:34.309687Z","shell.execute_reply.started":"2024-12-17T03:31:34.304246Z","shell.execute_reply":"2024-12-17T03:31:34.308136Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"\n# Step 6: Prepare the tokenized Dataset\n\nIn this phase, we do the following tasks:\n\n  - Remove unnecessary columns such as the \"posts\" column from the tokenized dataset as we no longer need them\n  -  Change the format of the tokenized dataset into pytorch since we are using pytorch\n  - Load the dataset using DataLoader with the proper batch size\n  - Preview the features of the dataset to make sure everything is okay\n\n","metadata":{}},{"cell_type":"code","source":"\n#remove the posts column as it is no longer needed\ntokenized_datasets = tokenized_datasets.remove_columns([\"content\"])\n\n\n#changing the format of the tokenized dataset to torch\ntokenized_datasets.set_format(\"torch\")\n\n\n#shuffeling and selecting the needed size of dataset for training and evaluating the model\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(training_data_size))\nsmall_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(testing_data_size))\nsmall_eval_dataset = tokenized_datasets[\"evaluate\"].shuffle(seed=42).select(range(evaluation_data_size))\n\n\n# preview of the shuffeled and selected evaluation dataset\nsmall_eval_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:37.653624Z","iopub.execute_input":"2024-12-17T03:31:37.654056Z","iopub.status.idle":"2024-12-17T03:31:37.690854Z","shell.execute_reply.started":"2024-12-17T03:31:37.654014Z","shell.execute_reply":"2024-12-17T03:31:37.689668Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 301\n})"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# preview of the shuffeled and selected training dataset\nsmall_train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:40.081654Z","iopub.execute_input":"2024-12-17T03:31:40.082091Z","iopub.status.idle":"2024-12-17T03:31:40.089556Z","shell.execute_reply.started":"2024-12-17T03:31:40.082054Z","shell.execute_reply":"2024-12-17T03:31:40.088393Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 1214\n})"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# preview of the shuffeled and selected testing dataset\nsmall_test_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:40.344467Z","iopub.execute_input":"2024-12-17T03:31:40.345305Z","iopub.status.idle":"2024-12-17T03:31:40.352104Z","shell.execute_reply.started":"2024-12-17T03:31:40.345266Z","shell.execute_reply":"2024-12-17T03:31:40.350951Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 380\n})"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"#load the dataset using DataLoader\ntrain_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=4)\neval_dataloader = DataLoader(small_eval_dataset, batch_size=4)\ntest_dataloader = DataLoader(small_test_dataset, batch_size=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:40.536292Z","iopub.execute_input":"2024-12-17T03:31:40.536748Z","iopub.status.idle":"2024-12-17T03:31:40.542977Z","shell.execute_reply.started":"2024-12-17T03:31:40.536708Z","shell.execute_reply":"2024-12-17T03:31:40.541790Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"# Step 7: Fine-tune the model\n\nThis phase has the following steps:\n\n  - Load the model\n  - Specify the computing metric\n  - Specify the Training/fine-tuning arguments\n  - Load the Trainer class\n  - Fine-tune the model\n\n## 7.1 Load the model\nWe load the fine-tuned mBERT mode in this step\n Step 7: Fine-tune the model\n\nThis phase has the following steps:\n\n  - Load the model\n  - Specify the computing metric\n  - Specify the Training/fine-tuning arguments\n  - Load the Trainer class\n  - Fine-tune the model\n\n\n","metadata":{}},{"cell_type":"code","source":"#Load auto mode classifier from the pretrained model\nfrom transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Davlan/bert-base-multilingual-cased-finetuned-amharic\", num_labels=2)\n# model = AutoModelForSequenceClassification.from_pretrained(\"Davlan/\", num_labels=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:44.845393Z","iopub.execute_input":"2024-12-17T03:31:44.846387Z","iopub.status.idle":"2024-12-17T03:31:49.314256Z","shell.execute_reply.started":"2024-12-17T03:31:44.846326Z","shell.execute_reply":"2024-12-17T03:31:49.312961Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/712M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b559b8e7a014b3f9f2aa512412c72b5"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Davlan/bert-base-multilingual-cased-finetuned-amharic and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"## 7.2 Computing Metrics\nIn this stage, we load the computing metrics. The computing metrics used in this phase are the f1-score and the accuracy. These computing metrics are used during the validation and testing phase\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport evaluate\n\n\n# Load the desired metrics\nf1_metric = evaluate.load(\"f1\")\naccuracy_metric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n\n    # Compute each metric\n    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n\n    return {\"f1\": f1, \"accuracy\": accuracy}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:51.321396Z","iopub.execute_input":"2024-12-17T03:31:51.321848Z","iopub.status.idle":"2024-12-17T03:31:52.082529Z","shell.execute_reply.started":"2024-12-17T03:31:51.321809Z","shell.execute_reply":"2024-12-17T03:31:52.081323Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"948f1b35e3634d778640094614c42bbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edf353f281984218a86dc7441eaf3dd3"}},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"## 7.2 Computing Metrics\nIn this stage, we load the computing metrics. The computing metrics used in this phase are the f1-score and the accuracy. These computing metrics are used during the validation and testing phase","metadata":{}},{"cell_type":"code","source":"\n# #load an optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(output_dir=\"test_trainer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:55.641621Z","iopub.execute_input":"2024-12-17T03:31:55.642035Z","iopub.status.idle":"2024-12-17T03:31:57.025003Z","shell.execute_reply.started":"2024-12-17T03:31:55.642000Z","shell.execute_reply":"2024-12-17T03:31:57.023752Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\nfrom transformers import EarlyStoppingCallback, IntervalStrategy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:58.401760Z","iopub.execute_input":"2024-12-17T03:31:58.402171Z","iopub.status.idle":"2024-12-17T03:31:58.423431Z","shell.execute_reply.started":"2024-12-17T03:31:58.402134Z","shell.execute_reply":"2024-12-17T03:31:58.422055Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"\ntraining_args = TrainingArguments(\n   f\"training_with_callbacks\",\n   evaluation_strategy = IntervalStrategy.STEPS, # \"steps\"\n   warmup_steps=500,                # number of warmup steps for learning rate\n   save_steps=2000,\n   eval_steps = 2000, # Evaluation and Save happens every 50 steps\n   save_total_limit = 3, # Only last 5 models are saved. Older ones are deleted.\n   learning_rate=1e-5,\n   per_device_train_batch_size=4,\n   per_device_eval_batch_size=4,\n   num_train_epochs=10,\n   weight_decay=0.01,\n   push_to_hub=False,\n   metric_for_best_model = 'f1',\n   load_best_model_at_end=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:32:01.501495Z","iopub.execute_input":"2024-12-17T03:32:01.501899Z","iopub.status.idle":"2024-12-17T03:32:01.514797Z","shell.execute_reply.started":"2024-12-17T03:32:01.501860Z","shell.execute_reply":"2024-12-17T03:32:01.513539Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"## 7.4 Load the Trainer class\nIn the trainer class, early stopping strategy is called. Early Stopping is a an optimization technique used to reduce overfitting without compromising on model accuracy. It allows to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset. For this model, the early stopping patience used is 10 epoches.","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n    callbacks = [EarlyStoppingCallback(early_stopping_patience=10)],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:32:04.241934Z","iopub.execute_input":"2024-12-17T03:32:04.242378Z","iopub.status.idle":"2024-12-17T03:32:05.990830Z","shell.execute_reply.started":"2024-12-17T03:32:04.242316Z","shell.execute_reply":"2024-12-17T03:32:05.989761Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"## 7.5 Fine-tune the model\nFine-tuning process embbeds the validation within itself. After every 2000 steps of finetuning, the model is validated on the loaded computing metrics to modify the hyperparameters to make the model perform well","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:32:06.741488Z","iopub.execute_input":"2024-12-17T03:32:06.741950Z","iopub.status.idle":"2024-12-17T14:51:55.807399Z","shell.execute_reply.started":"2024-12-17T03:32:06.741913Z","shell.execute_reply":"2024-12-17T14:51:55.804704Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241217_033222-rsnxgqqf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/shuaibahmedin-icog-labs/huggingface/runs/rsnxgqqf' target=\"_blank\">training_with_callbacks</a></strong> to <a href='https://wandb.ai/shuaibahmedin-icog-labs/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/shuaibahmedin-icog-labs/huggingface' target=\"_blank\">https://wandb.ai/shuaibahmedin-icog-labs/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/shuaibahmedin-icog-labs/huggingface/runs/rsnxgqqf' target=\"_blank\">https://wandb.ai/shuaibahmedin-icog-labs/huggingface/runs/rsnxgqqf</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3040' max='3040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3040/3040 11:19:14, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2000</td>\n      <td>0.024400</td>\n      <td>1.005247</td>\n      <td>0.876543</td>\n      <td>0.867110</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3040, training_loss=0.16095110881295496, metrics={'train_runtime': 40788.6296, 'train_samples_per_second': 0.298, 'train_steps_per_second': 0.075, 'total_flos': 3194168212070400.0, 'train_loss': 0.16095110881295496, 'epoch': 10.0})"},"metadata":{}}],"execution_count":36},{"cell_type":"markdown","source":"\n# Step 8: Test the model\n\nIn this stage the model is tested on the testing dataset. This dataset isn't seen by the model during the finetuning process.","metadata":{}},{"cell_type":"code","source":"trainer.evaluate(small_test_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T14:52:28.736602Z","iopub.execute_input":"2024-12-17T14:52:28.737539Z","iopub.status.idle":"2024-12-17T14:57:57.524973Z","shell.execute_reply.started":"2024-12-17T14:52:28.737494Z","shell.execute_reply":"2024-12-17T14:57:57.523779Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [95/95 05:25]\n    </div>\n    "},"metadata":{}},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 1.057020664215088,\n 'eval_f1': 0.8697788697788698,\n 'eval_accuracy': 0.8605263157894737,\n 'eval_runtime': 328.7208,\n 'eval_samples_per_second': 1.156,\n 'eval_steps_per_second': 0.289,\n 'epoch': 10.0}"},"metadata":{}}],"execution_count":38},{"cell_type":"markdown","source":"# 1. Save Your Fine-Tuned Model Locally\n\nAfter fine-tuning, your model and tokenizer reside in memory. Save them to a directory for reuse.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport os\n\n# Replace `model` and `tokenizer` with your fine-tuned ones\n# Specify the directory where you want to save the model\nsave_directory = \"/kaggle/working/saved_model\"\n\n# Save the model and tokenizer\nmodel.save_pretrained(save_directory)\ntokenizer.save_pretrained(save_directory)\n\nprint(f\"Model and tokenizer saved to: {save_directory}\")\nprint(\"\\nVerifying saved files...\")\nprint(os.listdir(save_directory))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:05:02.478493Z","iopub.execute_input":"2024-12-17T15:05:02.478976Z","iopub.status.idle":"2024-12-17T15:05:04.184015Z","shell.execute_reply.started":"2024-12-17T15:05:02.478929Z","shell.execute_reply":"2024-12-17T15:05:04.182960Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer saved to: /kaggle/working/saved_model\n\nVerifying saved files...\n['tokenizer.json', 'config.json', 'vocab.txt', 'model.safetensors', 'tokenizer_config.json', 'special_tokens_map.json']\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"\n# Step 9: Push the model to Huggingface Hub\n\nOne of the aims of this research/project is to contribute to the IT community in the sector of NLP tasks on low-resourced languages. Hence, the final model was pushed and made publicly available on Huggingface. You can find the model on huggingface here","metadata":{}},{"cell_type":"code","source":"#install huggingface_hub package to interact with huggingface platform\n!pip install huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:22:49.357024Z","iopub.execute_input":"2024-12-17T15:22:49.357626Z","iopub.status.idle":"2024-12-17T15:22:59.950694Z","shell.execute_reply.started":"2024-12-17T15:22:49.357568Z","shell.execute_reply":"2024-12-17T15:22:59.949505Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\n#login to huggingface\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:24:28.126176Z","iopub.execute_input":"2024-12-17T15:24:28.127061Z","iopub.status.idle":"2024-12-17T15:24:28.151451Z","shell.execute_reply.started":"2024-12-17T15:24:28.127008Z","shell.execute_reply":"2024-12-17T15:24:28.150508Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9b987ce14ff4d5ea412cc110ba80a3a"}},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"#loading a tokenizer from the pretrained model\ntokenizer = AutoTokenizer.from_pretrained(\"Davlan/bert-base-multilingual-cased-finetuned-amharic\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:24:56.255168Z","iopub.execute_input":"2024-12-17T15:24:56.256114Z","iopub.status.idle":"2024-12-17T15:24:56.683375Z","shell.execute_reply.started":"2024-12-17T15:24:56.256073Z","shell.execute_reply":"2024-12-17T15:24:56.682558Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nloaded_model = AutoModelForSequenceClassification.from_pretrained(save_directory)\nloaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n\n# model = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/fine-tuned_BERT/saved_model\")\n\n#specify the path for the trainde model and tokenizer to huggingface repository\nloaded_model.save_pretrained(\"path/to/amharic-hate-speech-detection-mBERT\")\nloaded_tokenizer.save_pretrained(\"path/to/amharic-hate-speech-detection-mBERT\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:24:59.846110Z","iopub.execute_input":"2024-12-17T15:24:59.847102Z","iopub.status.idle":"2024-12-17T15:25:02.135062Z","shell.execute_reply.started":"2024-12-17T15:24:59.847061Z","shell.execute_reply":"2024-12-17T15:25:02.134179Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"('path/to/amharic-hate-speech-detection-mBERT/tokenizer_config.json',\n 'path/to/amharic-hate-speech-detection-mBERT/special_tokens_map.json',\n 'path/to/amharic-hate-speech-detection-mBERT/vocab.txt',\n 'path/to/amharic-hate-speech-detection-mBERT/added_tokens.json',\n 'path/to/amharic-hate-speech-detection-mBERT/tokenizer.json')"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"\n#push the trained model to huggingface repository\nloaded_model.push_to_hub(\"amharic-hate-speech-detection-mBERT\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:25:06.333864Z","iopub.execute_input":"2024-12-17T15:25:06.334586Z","iopub.status.idle":"2024-12-17T15:25:29.124104Z","shell.execute_reply.started":"2024-12-17T15:25:06.334547Z","shell.execute_reply":"2024-12-17T15:25:29.123018Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3767e18b57334506b51278cf3534c369"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/711M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f3069ddba7d4afc9b1924811dbab398"}},"metadata":{}},{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/shuaibam/amharic-hate-speech-detection-mBERT/commit/9aee948d1ca694fa917a4a89da03ac158063000f', commit_message='Upload BertForSequenceClassification', commit_description='', oid='9aee948d1ca694fa917a4a89da03ac158063000f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/shuaibam/amharic-hate-speech-detection-mBERT', endpoint='https://huggingface.co', repo_type='model', repo_id='shuaibam/amharic-hate-speech-detection-mBERT'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"#push the tokenizer to huggingface repository\nloaded_tokenizer.push_to_hub(\"amharic-hate-speech-detection-mBERT\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:25:39.925634Z","iopub.execute_input":"2024-12-17T15:25:39.926033Z","iopub.status.idle":"2024-12-17T15:25:41.192035Z","shell.execute_reply.started":"2024-12-17T15:25:39.925999Z","shell.execute_reply":"2024-12-17T15:25:41.191227Z"}},"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/shuaibam/amharic-hate-speech-detection-mBERT/commit/6392ecea556fb45921705efe14917db96248435c', commit_message='Upload tokenizer', commit_description='', oid='6392ecea556fb45921705efe14917db96248435c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/shuaibam/amharic-hate-speech-detection-mBERT', endpoint='https://huggingface.co', repo_type='model', repo_id='shuaibam/amharic-hate-speech-detection-mBERT'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"loaded_model.save_pretrained(save_directory)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:27:02.246967Z","iopub.execute_input":"2024-12-17T15:27:02.247460Z","iopub.status.idle":"2024-12-17T15:27:04.927609Z","shell.execute_reply.started":"2024-12-17T15:27:02.247417Z","shell.execute_reply":"2024-12-17T15:27:04.926668Z"}},"outputs":[],"execution_count":60}]}