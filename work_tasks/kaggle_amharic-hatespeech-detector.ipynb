{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10215941,"sourceType":"datasetVersion","datasetId":6314604}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mirahaem/amharic-hatespeech-detector?scriptVersionId=213444778\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"#installing the datasets package\n!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:28:26.042396Z","iopub.execute_input":"2024-12-17T03:28:26.042823Z","iopub.status.idle":"2024-12-17T03:28:38.211263Z","shell.execute_reply.started":"2024-12-17T03:28:26.04278Z","shell.execute_reply":"2024-12-17T03:28:38.209994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#installing the transformers package\n!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:29:46.742516Z","iopub.execute_input":"2024-12-17T03:29:46.742939Z","iopub.status.idle":"2024-12-17T03:29:56.791768Z","shell.execute_reply.started":"2024-12-17T03:29:46.742902Z","shell.execute_reply":"2024-12-17T03:29:56.79024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#installing the evaluate package\n! pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:30:04.694006Z","iopub.execute_input":"2024-12-17T03:30:04.694586Z","iopub.status.idle":"2024-12-17T03:30:15.460583Z","shell.execute_reply.started":"2024-12-17T03:30:04.694529Z","shell.execute_reply":"2024-12-17T03:30:15.459336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#importing the datasets package\nfrom datasets import Dataset\nimport datasets\nimport evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:30:19.561842Z","iopub.execute_input":"2024-12-17T03:30:19.562277Z","iopub.status.idle":"2024-12-17T03:30:42.068803Z","shell.execute_reply.started":"2024-12-17T03:30:19.562237Z","shell.execute_reply":"2024-12-17T03:30:42.067405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#import numpy and pandas for mathematical computation and data manipulation respectively\nimport numpy as np\nimport pandas as pd\n#import the pipeline of transformers\nfrom transformers import pipeline\n#import AutoTokenizer for tokenization purposes\nfrom transformers import AutoTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:30:52.521822Z","iopub.execute_input":"2024-12-17T03:30:52.522249Z","iopub.status.idle":"2024-12-17T03:30:52.528453Z","shell.execute_reply.started":"2024-12-17T03:30:52.52221Z","shell.execute_reply":"2024-12-17T03:30:52.527217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import torch\nimport torch\n#import Data loader from torch\nfrom torch.utils.data import DataLoader\n#import an optimizer\nfrom torch.optim import AdamW\n#import tqdm for a progress bar\nfrom tqdm.auto import tqdm\n\n\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:30:58.081794Z","iopub.execute_input":"2024-12-17T03:30:58.082215Z","iopub.status.idle":"2024-12-17T03:30:58.087992Z","shell.execute_reply.started":"2024-12-17T03:30:58.082179Z","shell.execute_reply":"2024-12-17T03:30:58.086804Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Step 3: Import the dataset to be used for Training the model\n\nThe dataset used for this project is an Amharic dataset that was made available by Data Mendeley. It contains Amharic posts and comments retrieved from Facebook. It has 30,000 rows. The dataset can be accessed from [here](https://data.mendeley.com/datasets/ymtmxx385m)\n","metadata":{}},{"cell_type":"code","source":"\n# Paths to the uploaded datasets\ntest_data_path = '/kaggle/input/amhsdatasets/AMHSDataTest.txt'  # Replace with the name of your uploaded test file\ntrain_data_path = '/kaggle/input/amhsdatasets/AMHSDataTrain(1).txt'  # Replace with the name of your uploaded train file\n\n# Load and process the test dataset\nTest = pd.read_csv(test_data_path, header=None, names=['raw'])\nTest['content'] = Test['raw'].apply(lambda x: ','.join(x.split(',')[:-1]))\nTest['label'] = Test['raw'].apply(lambda x: x.split(',')[-1])\n\n# Load and process the train dataset\nTrain = pd.read_csv(train_data_path, header=None, names=['raw'])\nTrain['content'] = Train['raw'].apply(lambda x: ','.join(x.split(',')[:-1]))\nTrain['label'] = Train['raw'].apply(lambda x: x.split(',')[-1])\n\n# Drop the raw column (optional)\nTrain = Train.drop(columns=['raw'])\nTest = Test.drop(columns=['raw'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:05.583547Z","iopub.execute_input":"2024-12-17T03:31:05.583938Z","iopub.status.idle":"2024-12-17T03:31:05.625269Z","shell.execute_reply.started":"2024-12-17T03:31:05.583904Z","shell.execute_reply":"2024-12-17T03:31:05.624231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the processed DataFrame\n(Train.head(-10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:06.502539Z","iopub.execute_input":"2024-12-17T03:31:06.502951Z","iopub.status.idle":"2024-12-17T03:31:06.523783Z","shell.execute_reply.started":"2024-12-17T03:31:06.502916Z","shell.execute_reply":"2024-12-17T03:31:06.522442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Test.head(-10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:08.163051Z","iopub.execute_input":"2024-12-17T03:31:08.163454Z","iopub.status.idle":"2024-12-17T03:31:08.175474Z","shell.execute_reply.started":"2024-12-17T03:31:08.163417Z","shell.execute_reply":"2024-12-17T03:31:08.17439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Train['label'] = Train['label'].replace(['መልካም', 'ጥላቻ ','ጥላቻ'],[0,1,1])\n(Train.head(-10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:09.702814Z","iopub.execute_input":"2024-12-17T03:31:09.703192Z","iopub.status.idle":"2024-12-17T03:31:09.718085Z","shell.execute_reply.started":"2024-12-17T03:31:09.703159Z","shell.execute_reply":"2024-12-17T03:31:09.716821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Test['label'] = Test['label'].replace(['መልካም', 'መልካም', 'መልካም', 'ጥላቻ ','ጥላቻ'], [0, 0, 0, 1,  1])\nTest.head(-10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:11.876478Z","iopub.execute_input":"2024-12-17T03:31:11.876891Z","iopub.status.idle":"2024-12-17T03:31:11.89126Z","shell.execute_reply.started":"2024-12-17T03:31:11.876852Z","shell.execute_reply":"2024-12-17T03:31:11.889979Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Step 4: Preprocess the Dataset\n\nWhen the dataset was retrived, the labels and the post were in different files.\n\n  - Hence, the first step in this phase is merging the files into one panda's dataframe.\n  - Second step is Label encoding. Lable encoding is the process of converting the labels(classes) into numeric format to make it easier for the machine to understand it\n  - Third step is dividing the dataset into training, validation and testing categories. The division ratio is 7:1:2 respectively.\n  - Last step is to remove an unncessary columns from the main dataset and merging the all the categories into one main dataset\n\n","metadata":{}},{"cell_type":"code","source":"Test['label'] = Test['label'].replace({'መልካም': 0})\nTest['label'] = Test['label'].replace({'ጥላቻ': 1})\nTest = Test[Test['label'] != 'Label']\nTest['label'] = pd.to_numeric(Test['label'], errors='coerce')  # Invalid entries become NaN\nTest = Test.dropna(subset=['label'])\nTest['label'] = Test['label'].astype(int)\nTest.head(-10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:13.342481Z","iopub.execute_input":"2024-12-17T03:31:13.342991Z","iopub.status.idle":"2024-12-17T03:31:13.365204Z","shell.execute_reply.started":"2024-12-17T03:31:13.342953Z","shell.execute_reply":"2024-12-17T03:31:13.363735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# trainn_dataset, evaluat_dataset = train_test_split(Train, test_size=0.75, random_state=42)\ntrain_dataset, evaluation_dataset = train_test_split(Train, test_size=0.20, random_state=42)\ntest_dataset, eval_test_dataset = train_test_split(Test, test_size=0.001, random_state=42)\nprint('Training dataset shape: ', train_dataset.shape)\nprint('Validation dataset shape: ', evaluation_dataset.shape)\nprint('Testing dataset shape: ', test_dataset.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:14.794385Z","iopub.execute_input":"2024-12-17T03:31:14.794794Z","iopub.status.idle":"2024-12-17T03:31:14.806272Z","shell.execute_reply.started":"2024-12-17T03:31:14.794758Z","shell.execute_reply":"2024-12-17T03:31:14.805165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#convert format of the dataset to HuggingFace Dataset from Pandas DataFrame\ntrain_dataset['label'] = train_dataset['label'].replace({'መልካም': 0})\ntrain_dataset['label'] = train_dataset['label'].replace({'ጥላቻ': 1})\ntrain_dataset = train_dataset[train_dataset['label'] != 'Label']\ntrain_dataset['label'] = pd.to_numeric(train_dataset['label'], errors='coerce')  # Invalid entries become NaN\ntrain_dataset = train_dataset.dropna(subset=['label'])\ntrain_dataset['label'] = train_dataset['label'].astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:15.062568Z","iopub.execute_input":"2024-12-17T03:31:15.063023Z","iopub.status.idle":"2024-12-17T03:31:15.077001Z","shell.execute_reply.started":"2024-12-17T03:31:15.062986Z","shell.execute_reply":"2024-12-17T03:31:15.075792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#convert format of the dataset to HuggingFace Dataset from Pandas DataFrame\nevaluation_dataset['label'] = evaluation_dataset['label'].replace({'መልካም': 0})\nevaluation_dataset['label'] = evaluation_dataset['label'].replace({'ጥላቻ': 1})\nevaluation_dataset = evaluation_dataset[evaluation_dataset['label'] != 'Label']\nevaluation_dataset['label'] = pd.to_numeric(evaluation_dataset['label'], errors='coerce')  # Invalid entries become NaN\nevaluation_dataset = evaluation_dataset.dropna(subset=['label'])\nevaluation_dataset['label'] = evaluation_dataset['label'].astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:15.62258Z","iopub.execute_input":"2024-12-17T03:31:15.623016Z","iopub.status.idle":"2024-12-17T03:31:15.635606Z","shell.execute_reply.started":"2024-12-17T03:31:15.622977Z","shell.execute_reply":"2024-12-17T03:31:15.634656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#convert format of the dataset to HuggingFace Dataset from Pandas DataFrame\ntest_dataset=Dataset.from_pandas(test_dataset)\n\n#convert the format of the dataset to HuggingFace Dataset from Pandas DataFrame\ntrain_dataset=Dataset.from_pandas(train_dataset)\n\n#convert the format of the dataset to HuggingFace Dataset from Pandas DataFrame\nevaluation_dataset=Dataset.from_pandas(evaluation_dataset)\n\n#preview of the dataset after conversion\n(test_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:15.902495Z","iopub.execute_input":"2024-12-17T03:31:15.903486Z","iopub.status.idle":"2024-12-17T03:31:15.958455Z","shell.execute_reply.started":"2024-12-17T03:31:15.903447Z","shell.execute_reply":"2024-12-17T03:31:15.957325Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#combine the train and test dataset into one datset\nmain_dataset= datasets.DatasetDict({\n    'train': train_dataset,\n    'test': test_dataset,\n    'evaluate': evaluation_dataset\n})\n\n#preview of the dataset after merging\nmain_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:17.463601Z","iopub.execute_input":"2024-12-17T03:31:17.464442Z","iopub.status.idle":"2024-12-17T03:31:17.471847Z","shell.execute_reply.started":"2024-12-17T03:31:17.46439Z","shell.execute_reply":"2024-12-17T03:31:17.470682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# training and testing data size\ntraining_data_size = main_dataset['train'].num_rows\ntesting_data_size = main_dataset['test'].num_rows\nevaluation_data_size = main_dataset['evaluate'].num_rows","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:17.726784Z","iopub.execute_input":"2024-12-17T03:31:17.727268Z","iopub.status.idle":"2024-12-17T03:31:17.732822Z","shell.execute_reply.started":"2024-12-17T03:31:17.727228Z","shell.execute_reply":"2024-12-17T03:31:17.731683Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 5: Tokenizing Dataset\n\n\nA Tokenizer is used to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data.\n\nIn this case, the tokenizer used is an AutoTokenizer from the fine-tuned mBERT model made available by Hugging face here\n\nIn this phase, we have the following tasks:\n\n- Load the tokenizer\n- Create a tokenizer function that takes the dataset in batches and tokenize them using the tokenizer loaded from the model\n- Call the tokenizer function on the whole dataset\n\n","metadata":{}},{"cell_type":"code","source":"# Specify a custom cache directory\ntokenizer = AutoTokenizer.from_pretrained(\n    \"Davlan/bert-base-multilingual-cased-finetuned-amharic\",\n    cache_dir=\"/kaggle/working/\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:21.762953Z","iopub.execute_input":"2024-12-17T03:31:21.76339Z","iopub.status.idle":"2024-12-17T03:31:22.90514Z","shell.execute_reply.started":"2024-12-17T03:31:21.763337Z","shell.execute_reply":"2024-12-17T03:31:22.903926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loading a tokenizer from the pretrained model\ntokenizer = AutoTokenizer.from_pretrained(\"Davlan/bert-base-multilingual-cased-finetuned-amharic\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:24.402605Z","iopub.execute_input":"2024-12-17T03:31:24.403014Z","iopub.status.idle":"2024-12-17T03:31:25.378101Z","shell.execute_reply.started":"2024-12-17T03:31:24.402978Z","shell.execute_reply":"2024-12-17T03:31:25.376987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Have a tokenizer function that uses the tokenizer\ndef tokenize_function(data):\n    return tokenizer(data[\"content\"], padding=\"max_length\", truncation=True)\n\n\n#Tokenize all the data using the mapping functionality\ntokenized_datasets = main_dataset.map(tokenize_function)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:26.68245Z","iopub.execute_input":"2024-12-17T03:31:26.682905Z","iopub.status.idle":"2024-12-17T03:31:28.456066Z","shell.execute_reply.started":"2024-12-17T03:31:26.682866Z","shell.execute_reply":"2024-12-17T03:31:28.454931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#empty cache\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:34.303551Z","iopub.execute_input":"2024-12-17T03:31:34.304287Z","iopub.status.idle":"2024-12-17T03:31:34.309687Z","shell.execute_reply.started":"2024-12-17T03:31:34.304246Z","shell.execute_reply":"2024-12-17T03:31:34.308136Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Step 6: Prepare the tokenized Dataset\n\nIn this phase, we do the following tasks:\n\n  - Remove unnecessary columns such as the \"posts\" column from the tokenized dataset as we no longer need them\n  -  Change the format of the tokenized dataset into pytorch since we are using pytorch\n  - Load the dataset using DataLoader with the proper batch size\n  - Preview the features of the dataset to make sure everything is okay\n\n","metadata":{}},{"cell_type":"code","source":"\n#remove the posts column as it is no longer needed\ntokenized_datasets = tokenized_datasets.remove_columns([\"content\"])\n\n\n#changing the format of the tokenized dataset to torch\ntokenized_datasets.set_format(\"torch\")\n\n\n#shuffeling and selecting the needed size of dataset for training and evaluating the model\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(training_data_size))\nsmall_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(testing_data_size))\nsmall_eval_dataset = tokenized_datasets[\"evaluate\"].shuffle(seed=42).select(range(evaluation_data_size))\n\n\n# preview of the shuffeled and selected evaluation dataset\nsmall_eval_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:37.653624Z","iopub.execute_input":"2024-12-17T03:31:37.654056Z","iopub.status.idle":"2024-12-17T03:31:37.690854Z","shell.execute_reply.started":"2024-12-17T03:31:37.654014Z","shell.execute_reply":"2024-12-17T03:31:37.689668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# preview of the shuffeled and selected training dataset\nsmall_train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:40.081654Z","iopub.execute_input":"2024-12-17T03:31:40.082091Z","iopub.status.idle":"2024-12-17T03:31:40.089556Z","shell.execute_reply.started":"2024-12-17T03:31:40.082054Z","shell.execute_reply":"2024-12-17T03:31:40.088393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# preview of the shuffeled and selected testing dataset\nsmall_test_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:40.344467Z","iopub.execute_input":"2024-12-17T03:31:40.345305Z","iopub.status.idle":"2024-12-17T03:31:40.352104Z","shell.execute_reply.started":"2024-12-17T03:31:40.345266Z","shell.execute_reply":"2024-12-17T03:31:40.350951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#load the dataset using DataLoader\ntrain_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=4)\neval_dataloader = DataLoader(small_eval_dataset, batch_size=4)\ntest_dataloader = DataLoader(small_test_dataset, batch_size=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:40.536292Z","iopub.execute_input":"2024-12-17T03:31:40.536748Z","iopub.status.idle":"2024-12-17T03:31:40.542977Z","shell.execute_reply.started":"2024-12-17T03:31:40.536708Z","shell.execute_reply":"2024-12-17T03:31:40.54179Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 7: Fine-tune the model\n\nThis phase has the following steps:\n\n  - Load the model\n  - Specify the computing metric\n  - Specify the Training/fine-tuning arguments\n  - Load the Trainer class\n  - Fine-tune the model\n\n## 7.1 Load the model\nWe load the fine-tuned mBERT mode in this step\n Step 7: Fine-tune the model\n\nThis phase has the following steps:\n\n  - Load the model\n  - Specify the computing metric\n  - Specify the Training/fine-tuning arguments\n  - Load the Trainer class\n  - Fine-tune the model\n\n\n","metadata":{}},{"cell_type":"code","source":"#Load auto mode classifier from the pretrained model\nfrom transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Davlan/bert-base-multilingual-cased-finetuned-amharic\", num_labels=2)\n# model = AutoModelForSequenceClassification.from_pretrained(\"Davlan/\", num_labels=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:44.845393Z","iopub.execute_input":"2024-12-17T03:31:44.846387Z","iopub.status.idle":"2024-12-17T03:31:49.314256Z","shell.execute_reply.started":"2024-12-17T03:31:44.846326Z","shell.execute_reply":"2024-12-17T03:31:49.312961Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.2 Computing Metrics\nIn this stage, we load the computing metrics. The computing metrics used in this phase are the f1-score and the accuracy. These computing metrics are used during the validation and testing phase\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport evaluate\n\n\n# Load the desired metrics\nf1_metric = evaluate.load(\"f1\")\naccuracy_metric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n\n    # Compute each metric\n    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n\n    return {\"f1\": f1, \"accuracy\": accuracy}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:51.321396Z","iopub.execute_input":"2024-12-17T03:31:51.321848Z","iopub.status.idle":"2024-12-17T03:31:52.082529Z","shell.execute_reply.started":"2024-12-17T03:31:51.321809Z","shell.execute_reply":"2024-12-17T03:31:52.081323Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.2 Computing Metrics\nIn this stage, we load the computing metrics. The computing metrics used in this phase are the f1-score and the accuracy. These computing metrics are used during the validation and testing phase","metadata":{}},{"cell_type":"code","source":"\n# #load an optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(output_dir=\"test_trainer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:55.641621Z","iopub.execute_input":"2024-12-17T03:31:55.642035Z","iopub.status.idle":"2024-12-17T03:31:57.025003Z","shell.execute_reply.started":"2024-12-17T03:31:55.642Z","shell.execute_reply":"2024-12-17T03:31:57.023752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\nfrom transformers import EarlyStoppingCallback, IntervalStrategy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:31:58.40176Z","iopub.execute_input":"2024-12-17T03:31:58.402171Z","iopub.status.idle":"2024-12-17T03:31:58.423431Z","shell.execute_reply.started":"2024-12-17T03:31:58.402134Z","shell.execute_reply":"2024-12-17T03:31:58.422055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntraining_args = TrainingArguments(\n   f\"training_with_callbacks\",\n   evaluation_strategy = IntervalStrategy.STEPS, # \"steps\"\n   warmup_steps=500,                # number of warmup steps for learning rate\n   save_steps=2000,\n   eval_steps = 2000, # Evaluation and Save happens every 50 steps\n   save_total_limit = 3, # Only last 5 models are saved. Older ones are deleted.\n   learning_rate=1e-5,\n   per_device_train_batch_size=4,\n   per_device_eval_batch_size=4,\n   num_train_epochs=10,\n   weight_decay=0.01,\n   push_to_hub=False,\n   metric_for_best_model = 'f1',\n   load_best_model_at_end=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:32:01.501495Z","iopub.execute_input":"2024-12-17T03:32:01.501899Z","iopub.status.idle":"2024-12-17T03:32:01.514797Z","shell.execute_reply.started":"2024-12-17T03:32:01.50186Z","shell.execute_reply":"2024-12-17T03:32:01.513539Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.4 Load the Trainer class\nIn the trainer class, early stopping strategy is called. Early Stopping is a an optimization technique used to reduce overfitting without compromising on model accuracy. It allows to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset. For this model, the early stopping patience used is 10 epoches.","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n    callbacks = [EarlyStoppingCallback(early_stopping_patience=10)],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:32:04.241934Z","iopub.execute_input":"2024-12-17T03:32:04.242378Z","iopub.status.idle":"2024-12-17T03:32:05.99083Z","shell.execute_reply.started":"2024-12-17T03:32:04.242316Z","shell.execute_reply":"2024-12-17T03:32:05.989761Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.5 Fine-tune the model\nFine-tuning process embbeds the validation within itself. After every 2000 steps of finetuning, the model is validated on the loaded computing metrics to modify the hyperparameters to make the model perform well","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T03:32:06.741488Z","iopub.execute_input":"2024-12-17T03:32:06.74195Z","iopub.status.idle":"2024-12-17T14:51:55.807399Z","shell.execute_reply.started":"2024-12-17T03:32:06.741913Z","shell.execute_reply":"2024-12-17T14:51:55.804704Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Step 8: Test the model\n\nIn this stage the model is tested on the testing dataset. This dataset isn't seen by the model during the finetuning process.","metadata":{}},{"cell_type":"code","source":"trainer.evaluate(small_test_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T14:52:28.736602Z","iopub.execute_input":"2024-12-17T14:52:28.737539Z","iopub.status.idle":"2024-12-17T14:57:57.524973Z","shell.execute_reply.started":"2024-12-17T14:52:28.737494Z","shell.execute_reply":"2024-12-17T14:57:57.523779Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Save Your Fine-Tuned Model Locally\n\nAfter fine-tuning, your model and tokenizer reside in memory. Save them to a directory for reuse.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport os\n\n# Replace `model` and `tokenizer` with your fine-tuned ones\n# Specify the directory where you want to save the model\nsave_directory = \"/kaggle/working/saved_model\"\n\n# Save the model and tokenizer\nmodel.save_pretrained(save_directory)\ntokenizer.save_pretrained(save_directory)\n\nprint(f\"Model and tokenizer saved to: {save_directory}\")\nprint(\"\\nVerifying saved files...\")\nprint(os.listdir(save_directory))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:05:02.478493Z","iopub.execute_input":"2024-12-17T15:05:02.478976Z","iopub.status.idle":"2024-12-17T15:05:04.184015Z","shell.execute_reply.started":"2024-12-17T15:05:02.478929Z","shell.execute_reply":"2024-12-17T15:05:04.18296Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Step 9: Push the model to Huggingface Hub\n\nOne of the aims of this research/project is to contribute to the IT community in the sector of NLP tasks on low-resourced languages. Hence, the final model was pushed and made publicly available on Huggingface. You can find the model on huggingface here","metadata":{}},{"cell_type":"code","source":"#install huggingface_hub package to interact with huggingface platform\n!pip install huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:22:49.357024Z","iopub.execute_input":"2024-12-17T15:22:49.357626Z","iopub.status.idle":"2024-12-17T15:22:59.950694Z","shell.execute_reply.started":"2024-12-17T15:22:49.357568Z","shell.execute_reply":"2024-12-17T15:22:59.949505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\n#login to huggingface\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:24:28.126176Z","iopub.execute_input":"2024-12-17T15:24:28.127061Z","iopub.status.idle":"2024-12-17T15:24:28.151451Z","shell.execute_reply.started":"2024-12-17T15:24:28.127008Z","shell.execute_reply":"2024-12-17T15:24:28.150508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loading a tokenizer from the pretrained model\ntokenizer = AutoTokenizer.from_pretrained(\"Davlan/bert-base-multilingual-cased-finetuned-amharic\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:24:56.255168Z","iopub.execute_input":"2024-12-17T15:24:56.256114Z","iopub.status.idle":"2024-12-17T15:24:56.683375Z","shell.execute_reply.started":"2024-12-17T15:24:56.256073Z","shell.execute_reply":"2024-12-17T15:24:56.682558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nloaded_model = AutoModelForSequenceClassification.from_pretrained(save_directory)\nloaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n\n# model = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/fine-tuned_BERT/saved_model\")\n\n#specify the path for the trainde model and tokenizer to huggingface repository\nloaded_model.save_pretrained(\"path/to/amharic-hate-speech-detection-mBERT\")\nloaded_tokenizer.save_pretrained(\"path/to/amharic-hate-speech-detection-mBERT\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:24:59.84611Z","iopub.execute_input":"2024-12-17T15:24:59.847102Z","iopub.status.idle":"2024-12-17T15:25:02.135062Z","shell.execute_reply.started":"2024-12-17T15:24:59.847061Z","shell.execute_reply":"2024-12-17T15:25:02.134179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#push the trained model to huggingface repository\nloaded_model.push_to_hub(\"amharic-hate-speech-detection-mBERT\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:25:06.333864Z","iopub.execute_input":"2024-12-17T15:25:06.334586Z","iopub.status.idle":"2024-12-17T15:25:29.124104Z","shell.execute_reply.started":"2024-12-17T15:25:06.334547Z","shell.execute_reply":"2024-12-17T15:25:29.123018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#push the tokenizer to huggingface repository\nloaded_tokenizer.push_to_hub(\"amharic-hate-speech-detection-mBERT\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:25:39.925634Z","iopub.execute_input":"2024-12-17T15:25:39.926033Z","iopub.status.idle":"2024-12-17T15:25:41.192035Z","shell.execute_reply.started":"2024-12-17T15:25:39.925999Z","shell.execute_reply":"2024-12-17T15:25:41.191227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loaded_model.save_pretrained(save_directory)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:27:02.246967Z","iopub.execute_input":"2024-12-17T15:27:02.24746Z","iopub.status.idle":"2024-12-17T15:27:04.927609Z","shell.execute_reply.started":"2024-12-17T15:27:02.247417Z","shell.execute_reply":"2024-12-17T15:27:04.926668Z"}},"outputs":[],"execution_count":null}]}